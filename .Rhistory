gelman.diag(bayes.marginal)
# ------------------------------------------------------------
# now recode the factor designation for irrigation and density
# so that they are independent groups, nested within the
# multi-level structure
# cask interacts with block to produce "sample"
mydata$level1 <- as.numeric(Pastes$sample)
mydata$n_level1 <- max(mydata$level1)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
gelman.diag(bayes.nested)
# ------------------------------------------------------------
# Compare the various models
summary(m.nested)
summary(m.marginal)
summary(bayes.marginal)
summary(bayes.nested)
level.variances
hdrcde::hdr(as.matrix(bayes.nested))
nested.mat <- as.matrix(bayes.nested)
nested.modes <- apply(nested.mat, 2,
function(x){hdrcde::hdr(x)$mode})
for ( i in 1:4){
plot(hdrcde::hdr.den(nested.mat[,i], 50, main = colnames(nested.mat)[i]))
}
plot(bayes.nested)
?codamenu
densityplot(bayes.nested)
densityplot.mcmc(bayes.nested)
densplot(bayes.nested)
source('~/Documents/Projects/random-snippets/nested-only-random-effects-pastes.R', echo=TRUE)
pairs(bayes.nested)
pairs(as.matrix(bayes.nested)
)
source('~/Documents/Projects/random-snippets/nested-only-random-effects-pastes.R', echo=TRUE)
gelman.diag(bayes.nested)
# Comparison of encoding of nested random effects in lme4
# Example from The R Book by Michael Crawley
rm(list=ls())
# load the glmm package
library(lme4)
# load the rjags package
library(rjags)
library(viridis)
data("Pastes")
# ------------------------------------------------------------
# ------------------------------------------------------------
# Generate some summary statistics
# ------------------------------------------------------------
# the various means for each group at each level
overall.mean <- mean(Pastes$strength)
level.0.mu <- tapply(Pastes$strength, Pastes$sample, mean)
level.1.mu <- tapply(Pastes$strength, Pastes$cask, mean)
level.2.mu <- tapply(Pastes$strength, Pastes$batch, mean)
# the variances of those means (and the overall variance)
overall.var <- var(Pastes$strength)
level.0.var <- var(level.0.mu)
level.1.var <- var(level.1.mu)
level.2.var <- var(level.2.mu)
# collate the variances among means at each level
# i.e. these are the naive summary random effects.
level.variances <- c(L0 = level.0.var,
L1 = level.1.var,
L2 = level.2.var,
Overall = overall.var)
# for sake of summary, assume independence of variances, and sum them
# which *should* be close enough to the overall variance unless there
# is loads of covariance among the levels.
ind.var <- sum(level.variances[1:2])
# ------------------------------------------------------------
# PLOT THESE SUMMARY STATISTICS
# ------------------------------------------------------------
palette(viridis(length(level.2.mu)))
# number of observaitons in our dataset
nobs <- nrow(Pastes)
# for adding colour to the plot, and to track block all the way
# down to level 0, we extract the appropriate block code.
level.0.block <- Pastes$batch
level.1.block <- tapply(Pastes$batch, Pastes$sample, function(x){x[1]})
level.2.block <- tapply(Pastes$batch, Pastes$batch, function(x){x[1]})
jt <- 0.15
# plot the raw data, i.e. level 0, with a bit of jitter
plot(jitter(rep(0, nobs), amount = jt) ~ Pastes$strength,
col = level.0.block,
ylim = c(-0.1, 3.1),
xlim = c(min(Pastes$strength) - 1, max(Pastes$strength) + 1),
bty = "L",
yaxt = "n",
pch = 20)
axis(2, at = 0:3, las = 1)
# plot the means at level 1
points(jitter(rep(1, length(level.1.mu)), amount = jt) ~ level.1.mu,
col = level.1.block,
pch = 20)
# plot the means at level 2
points(jitter(rep(2, length(level.2.mu)), amount = jt) ~ level.2.mu,
col = level.2.block,
pch = 20)
# add the overall mean
abline(v = mean(Pastes$strength), lty = 2, lwd = 2, col = "grey")
# ------------------------------------------------------------
# FIT THE GLMS USING ML
# ------------------------------------------------------------
# nested random effects
m.nested <- lmer(strength ~ 1 + (1|batch/cask), data = Pastes)
# not-nested random effects
m.marginal <- lmer(strength ~ 1 + (1|batch) + (1|cask),
data = Pastes)
# ------------------------------------------------------------
# THE BAYESIAN GLM
# ------------------------------------------------------------
# Define the jags model as a string
model_string = '
model {
# Likelihood
for (i in 1:n){
y[i] ~ dnorm(mu[i], tau)
mu[i] <- b0 + U[level1[i]] + V[level2[i]]
}
# Random effect of level1 (cask:batch, i.e. sample)
for (j in 1:n_level1){
U[j] ~ dnorm(0, tau_level1)
}
# Random effect of level2 (batch)
for (k in 1:n_level2){
V[k] ~ dnorm(0, tau_level2)
}
# Calculate the total variance
v_tot <- v_level1 + v_level2
# ------------------------------------------------------------
# priors on fixed parts
# the intercept
b0 ~ dnorm(0, 10^-6)
# ------------------------------------------------------------
# priors on random parts
# these are specified on the standard deviations, and then
# variance and precision are calculated for each for the
# likelihood above.
sigma_level1 ~ dunif(0,10)
v_level1 <- sigma_level1 * sigma_level1
tau_level1 <- 1 / v_level1
sigma_level2 ~ dunif(0,10)
v_level2 <- sigma_level2 * sigma_level2
tau_level2 <- 1 / v_level2
sigma ~ dunif(0,10)
v_resid <- sigma * sigma
tau <- 1 / v_resid
} # end of model
'
# ------------------------------------------------------------
# collect the data required by the model in a list.
# This is where we also have to convert our factors into
# sequential integers.
mydata <- list()
mydata$y <- Pastes$strength
mydata$level1 <- as.numeric(Pastes$sample)
mydata$level2 <- as.numeric(Pastes$batch)
mydata$n_level1 <- max(mydata$level1)
mydata$n_level2 <- max(mydata$level2)
mydata$n <- length(Pastes$strength)
# cask interacts with block to produce "sample"
model = jags.model(textConnection(model_string),
data = mydata,
n.chain = 2,
n.adapt = 10000)
bayes.nested = coda.samples(model = model,
variable.names = c("b0",
"v_level1",
"v_level2",
"v_resid"),
n.iter = 5 * 10^4,
thin = 20)
# test for convergence. Which fails quite a bit for some reason
# with an error i dont understand.
gelman.diag(bayes.nested)
# density plots of the posteriors
densplot(bayes.nested)
# ------------------------------------------------------------
# Compare the various models
summary(m.nested)
summary(m.marginal)
summary(bayes.nested)
level.variances
hdrcde::hdr(as.matrix(bayes.nested))
nested.mat <- as.matrix(bayes.nested)
nested.modes <- apply(nested.mat, 2,
function(x){hdrcde::hdr(x)$mode})
for ( i in 1:4){
plot(hdrcde::hdr.den(nested.mat[,i], 50, main = colnames(nested.mat)[i]))
}
summary(m.nested)
summary(bayes.nested)
nested.modes
source('~/Documents/Projects/random-snippets/nested-only-random-effects-pastes.R', echo=TRUE)
summary(m.nested)
summary(bayes.nested)
level.variances
# Example of how to fit random effects models to nested data
# in the various syntaxes using lme4::lmer()
rm(list=ls())
# load the glmm package
library(lme4)
data("Pastes")
# ------------------------------------------------------------
# FIT THE GLMS USING ML
# ------------------------------------------------------------
# nested random effects as per the / notation
m1 <- lmer(strength ~ 1 + (1|batch/cask), data = Pastes)
# the same by manually specifying the interation term
m2 <- lmer(strength ~ 1 + (1|batch) + (1|batch:cask))
# the same if one had encoded the data correctly in the first place...
m3 <- lmer(strength ~ 1 + (1|batch) + (1|sample))
summary(m1)
summary(m2)
summary(m3)
# Example of how to fit random effects models to nested data
# in the various syntaxes using lme4::lmer()
rm(list=ls())
# load the glmm package
library(lme4)
data("Pastes")
# ------------------------------------------------------------
# FIT THE GLMS USING ML
# ------------------------------------------------------------
# nested random effects as per the / notation
m1 <- lmer(strength ~ 1 + (1|batch/cask), data = Pastes)
# the same by manually specifying the interation term
m2 <- lmer(strength ~ 1 + (1|batch) + (1|batch:cask), data = Pastes)
# the same if one had encoded the data correctly in the first place...
m3 <- lmer(strength ~ 1 + (1|batch) + (1|sample), data = Pastes)
summary(m1)
summary(m2)
summary(m3)
head(Pastes)
head(Pastes,10)
summary(m.nested)
summary(bayes.nested)
level.variances
hdrcde::hdr(as.matrix(bayes.nested))
nested.mat <- as.matrix(bayes.nested)
nested.modes <- apply(nested.mat, 2,
source('~/Documents/Projects/random-snippets/compare-nested-random-effects-jags-lme4.R', echo=TRUE)
)
nested.modes <- apply(nested.mat, 2,
function(x){hdrcde::hdr(x)$mode})
nested.modes
?mode
summary(m.nested)
summary(bayes.nested)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
summary(m.nested)
summary(bayes.nested)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
summary(m.nested)
summary(bayes.nested)
source('~/Documents/Projects/random-snippets/nesting-in-lmer.R', echo=TRUE)
coefs(m1)
coef(m1)
fixef(m1)
fixef(m2)
ranef(m1)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
nested.modes
level.variances
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
nested.modes
bayes.mat <- as.matrix(bayes.nested)
bayes.modes <- apply(bayes.mat, 2,
function(x){hdrcde::hdr(x)$mode})
bayes.median <- apply(bayes.mat, 2, stats::median)
bayes.mean <- apply(bayes.mat, 2, mean)
bayes.mean
bayes.mediaan
bayes.median
bayes.modes
level.variances
jt <- 0.15
# plot the raw data, i.e. level 0, with a bit of jitter
plot(jitter(rep(0, nobs), amount = jt) ~ Pastes$strength,
col = level.0.block,
ylim = c(-0.1, 3.1),
xlim = c(min(Pastes$strength) - 1, max(Pastes$strength) + 1),
bty = "L",
yaxt = "n",
pch = 20)
axis(2, at = 0:3, las = 1)
# plot the means at level 1
points(jitter(rep(1, length(level.1.mu)), amount = jt) ~ level.1.mu,
col = level.1.block,
pch = 20)
# plot the means at level 2
points(jitter(rep(2, length(level.2.mu)), amount = jt) ~ level.2.mu,
col = level.2.block,
pch = 20)
# add the overall mean
abline(v = mean(Pastes$strength), lty = 2, lwd = 2, col = "grey")
par(mfrow=c(1,1))
# plot the raw data, i.e. level 0, with a bit of jitter
plot(jitter(rep(0, nobs), amount = jt) ~ Pastes$strength,
col = level.0.block,
ylim = c(-0.1, 3.1),
xlim = c(min(Pastes$strength) - 1, max(Pastes$strength) + 1),
bty = "L",
yaxt = "n",
pch = 20)
axis(2, at = 0:3, las = 1)
# plot the means at level 1
points(jitter(rep(1, length(level.1.mu)), amount = jt) ~ level.1.mu,
col = level.1.block,
pch = 20)
# plot the means at level 2
points(jitter(rep(2, length(level.2.mu)), amount = jt) ~ level.2.mu,
col = level.2.block,
pch = 20)
# add the overall mean
abline(v = mean(Pastes$strength), lty = 2, lwd = 2, col = "grey")
overall.mean <- mean(Pastes$strength)
level.1.mu <- tapply(Pastes$strength, Pastes$sample, mean)
level.2.mu <- tapply(Pastes$strength, Pastes$batch, mean)
level.1.mu
overall.var <- var(Pastes$strength)
level.1.var <- var(level.1.mu)
level.2.var <- var(level.2.mu)
level.1.var
var(Pastes$strength) - (level.1.var + level.2.var)
?rm
?data(Pastes)
?Pastes
?lme4::Pastes
source('~/Documents/Projects/random-snippets/nested-random-effects-pastes.R', echo=TRUE)
summary(m.nested)
gelman.diag(bayes.nested)
?rjags
?coda
?coda::densplot
coda::densplot(bayes.nested)
?rjags
bayes.modes
rbind(bayes.modes, bayes.median, bayes.mean)
print(round(rbind(bayes.modes, bayes.median, bayes.mean),2))
summary(m.nested)
print(round(rbind(bayes.modes, bayes.median, bayes.mean),2))
?lme4
knit_with_parameters('~/Documents/Projects/random-snippets/random-effects-in-jags-and-lme4.Rmd')
?knitr
?require
aj <- summary(m.nested)
aj
names(aj)
aj$varcor
aj$varcor^2
aj$varcor
aj$varcor[[]]^2
aj$varcor[[1]]^2
aj$varcor$`cask:batch`^2
aj$varcor$`cask:batch
`
aj$varcor$`cask:batch`
names(aj)
aj$sigma
aj$vcov
VarCorr(aj)
VarCorr(m.nested)
VarCorr(m.nested)^2
bb<-VarCorr(m.nested)
str(bb)
?VarCorr
bb$vcoc
bb$vcov
bb$`cask:batch`
print(b,comp=c("Variance","Std.Dev."),digits=2)
print(bb,comp=c("Variance","Std.Dev."),digits=2)
print(bb,comp=c("Variance"),digits=2)
print(m.nested,comp=c("Variance"),digits=2)
vc <- VarCorr(m.nested)
print(vc,comp=c("Variance"), digits = 2)
vc <- VarCorr(m.nested)
print(vc,comp=c("Variance"), digits = 2)
?table
?residuals
library(viridis)
?viridis
?extractAIC()
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
if(!require(viridis)) install.packages("viridis")
library(viridis)
# Chunk 2: simluate
# set the random seed for reproducibility
set.seed(1)
# sample size per group
n <- 50 #20
# number of groups
g <- 100
G <- factor(rep(LETTERS[1:g], n))
# set the colour palette accordingly (NB i use the library viridis)
palette(viridis(g))
# a slope for each group taken from a normal distribution
mu.b <- 2
s.b <- 3
b <- rnorm(g, mu.b, s.b)
B <- rep(b, n) # and replicate them out for each sample
# an intercept for each group, taken from a normal distribution
mu.c <- 0
s.c <- 2
c <- rnorm(g, mu.c, s.c)
C <- rep(c, n) # and replicate them out for each sample
# The linear covariate X is uniform random between -2 and 2
X <- runif(n*g, -2, 2)
# define the residual noise
s.r <- 1
s <- rnorm(n*g, 0, s.r)
# calculate the reponse variable Y
Y <- X * B + C + s
# Plot the results
plot(Y ~ X, col = G)
dd <- data.frame(Y = Y, X = X, G = G)
dd.s <- split(dd, G)
for (i in 1:length(dd.s)){
abline(lm(Y~X, data = dd.s[[i]]), col = i)
}
# Chunk 3: model
# load the lme4 library
if(!require(lme4, quietly = TRUE)) install.packages("lme4")
library(lme4, quietly = TRUE)
# fixed effects only. 4 slopes and 4 intercepts using glm()
m1 <- glm(Y ~ X * G)
# one slope and random intercept
m2 <- lmer(Y ~ X + (1|G))
# 4 slopes and a random intercept term
m3 <- lmer(Y ~ X + X:G + (1|G))
# random slope and random intercept
m4 <- lmer(Y ~ X + (X|G))
# random slope and random intercept (uncorrelated)
m5 <- lmer(Y ~ X + (X||G))
# Chunk 4: population.values
# collate the specified values from the start
true.coefs <- data.frame(Intercept = c, Slopes.X = b)
print(round(true.coefs,2))
# It is also useful to compare these coefficients to those for Group A,
# since model fitting often compares the effect of the other Groups to this
# baseline, reference group.
relative.coefs <- data.frame(G.effects = c - c[1], Slopes.effects = b - b[1])
print(round(relative.coefs, 2))
# the sample means of these coefficients
print(round(colMeans(true.coefs),2))
# and the population means and variances (which we specified a priori)
cat("Population mean\n")
cat(c(mu.c, mu.b))
cat("Population variances\n")
cat(c(s.c, s.b) ^ 2)
# Chunk 5: compare
# pull out and re-organise the estimates for the glm
tmp <- coef(m1)
glm.est <- data.frame(Intercepts = c(tmp[1], tmp[3:(g+1)]),
Slopes = c(tmp[2], tmp[(g+2):(g+g)]))
rownames(glm.est) <- LETTERS[1:g]
print(glm.est)
# ------------------------------------------------------------
# get estimates for the lmer objects which is much easier
rand.intercepts.1.slope <- coef(m2)$G
print(rand.intercepts.1.slope)
tmp <- coef(m3)$G
rand.inter.4.slopes <- data.frame(Intercept = tmp$`(Intercept)`,
X = unlist(c(tmp[1,2], tmp[1,3:(g+1)] +
tmp[1,2])))
rownames(rand.inter.4.slopes) <- LETTERS[1:g]
print(rand.inter.4.slopes)
# Random slopes and intercepts
rand.slopes <- coef(m4)$G
print(rand.slopes)
# Random slopes and intercepts
rand.slopes.u <- coef(m5)$G
print(rand.slopes.u)
# Chunk 6: variances
# summary of the full random slopes and random intercept model
summary(m4)
# summary of the full uncorrelated random slopes and random intercept model
summary(m5)
# we simulated using standard deviations, so need to square them to get
# variances.
sim.vars <- c(Var.intercepts = s.c ^ 2,
Var.slopes = s.b ^ 2,
Resid.Var = s.r ^ 2)
# Chunk 7: aic
lapply(list(m1, m2, m3, m4, m5), extractAIC)
# Chunk 8: loglik
lapply(list(m1, m2, m3, m4, m5), logLik)
summary(m1)
?lmer
lapply(list(m1, m2, m3, m4, m5), extractAIC, REMl = F)
lapply(list(m1, m2, m3, m4, m5), logLik)
-lapply(list(m1, m2, m3, m4, m5), logLik)
-2*lapply(list(m1, m2, m3, m4, m5), logLik)
?VarCorr.merMod
library(nnet)
?nnet
update.packages(nnet)
update.packages("nnet")
library(nlme)
?nlm
?nls
install.packages("devtools")
devtools::install_github("bomeara/cv")
library(cv)
?CreateMarkdown
